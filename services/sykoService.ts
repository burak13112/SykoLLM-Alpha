import { Message } from '../types';
import { GoogleGenAI } from "@google/genai";

// ============================================================================
// üõ†Ô∏è NGROK BAƒûLANTI AYARLARI
// ============================================================================

const CUSTOM_MODEL_CONFIG = {
  baseURL: "https://atlantic-appear-optics-tub.trycloudflare.com/v1",
  modelId: "Qwen_Qwen3-0.6Be", 
  apiKey: "EMPTY" 
};

// ============================================================================
// üß† SYKO PERSONA AYARLARI
// ============================================================================

const SHARED_THINKING_PROTOCOL = `
    üß† THOUGHT PROCESS PROTOCOL (<think>):
    1. You have the ability to think before answering using the <think> tag.
    2. SOFT LIMIT: Keep your reasoning process efficient (aim for 512-1024 tokens).
    3. BRAKE: As soon as you reach a conclusion, close with </think> and output the final response immediately.
    4. ALWAYS use the <think> tag for complex queries or reasoning tasks to trigger the UI thinking panel.
`;

const NATURAL_LANGUAGE_PROTOCOL = `
    üó£Ô∏è TONE & STYLE GUIDE:
    1. **BE NATURAL:** Speak like a smart, cool human friend. Avoid academic, robotic, or overly formal language.
    2. **KEEP IT SIMPLE:** Use daily life language. Be concise and direct.
    3. **NO REPETITION:** DO NOT constantly say "As SykoLLM" or "SykoLLM-PRO here". Just answer the question.
    4. **NO FILLERS:** Avoid starting with "Sure!", "Here is the answer", "I can help with that". Dive straight into the value.
    5. **LANGUAGE:** Strictly stick to English or Turkish based on the user's input.
`;

const SYSTEM_PROMPTS: Record<string, string> = {
  'syko-v2.5': `
    You are SykoLLM V2.5.
    Identity: A helpful, quick-witted AI companion.
    ${NATURAL_LANGUAGE_PROTOCOL}
    ${SHARED_THINKING_PROTOCOL}
  `,
  'syko-v3-pro': `
    You are SykoLLM V3.0 PRO (Flash Edition).
    Identity: A highly intelligent, fast-thinking AI entity.
    ${NATURAL_LANGUAGE_PROTOCOL}
    ${SHARED_THINKING_PROTOCOL}
  `,
  'syko-super-pro': `
    You are SykoLLM SUPER PRO.
    Identity: The most advanced, deep-reasoning AI entity in the system.
    ${NATURAL_LANGUAGE_PROTOCOL}
    ${SHARED_THINKING_PROTOCOL}
    Note: Demonstrate superior logic and creativity.
  `
};

// ============================================================================
// üñºÔ∏è G√ñRSEL √úRETƒ∞M SERVƒ∞Sƒ∞ (SYKO VISION MODE)
// ============================================================================

export const generateSykoImage = async (modelId: string, prompt: string, referenceImages?: string[]): Promise<{ text: string, images: string[] }> => {
  
  // --- 1. NATIVE / LOCAL MODEL (Dokunmuyoruz, kendi mantƒ±ƒüƒ±yla √ßalƒ±≈üƒ±yor) ---
  if (modelId === 'syko-native') {
    if (CUSTOM_MODEL_CONFIG.baseURL.includes("CHANGE_THIS")) {
      throw new Error("Lokal model ayarlarƒ± yapƒ±lmamƒ±≈ü.");
    }
    
    try {
      const response = await fetch(`${CUSTOM_MODEL_CONFIG.baseURL}/images/generations`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${CUSTOM_MODEL_CONFIG.apiKey}`
        },
        body: JSON.stringify({
          prompt: prompt,
          n: 1,
          size: "1024x1024",
          response_format: "b64_json"
        })
      });

      if (!response.ok) {
        throw new Error("Native generation failed");
      }

      const data = await response.json();
      if (data.data && data.data.length > 0) {
         const imgData = data.data[0].b64_json || data.data[0].url;
         const finalImg = imgData.startsWith('http') ? imgData : `data:image/png;base64,${imgData}`;
         return { text: "Generated by Syko NATIVE:", images: [finalImg] };
      } else {
        throw new Error("No image data returned");
      }

    } catch (e) {
      throw new Error("Lokal model g√∂rsel √ºretimi ba≈üarƒ±sƒ±z oldu.");
    }
  }

  // --- 2. GOOGLE MODELS (BACKEND SWITCH LOGIC) ---
  const apiKey = process.env.API_KEY;
  if (!apiKey) throw new Error("API Key eksik!");

  const ai = new GoogleGenAI({ apiKey });

  let targetImageModel = '';
  let imageConfig: any = {};
  
  if (modelId === 'syko-v2.5') {
    targetImageModel = 'gemini-2.5-flash-image';
    imageConfig = { aspectRatio: '1:1' };
  } else if (modelId === 'syko-v3-pro') {
    targetImageModel = 'gemini-2.5-flash-image';
    imageConfig = { aspectRatio: '1:1' };
  } else if (modelId === 'syko-super-pro') {
    // SUPER PRO -> ƒ∞lk √∂nce en iyisini deneriz
    targetImageModel = 'gemini-3-pro-image-preview';
    imageConfig = { aspectRatio: '1:1', imageSize: '1K' };
  } else {
    throw new Error("Se√ßili model g√∂rsel √ºretimi desteklemiyor.");
  }

  const parts: any[] = [];
  
  // Referans g√∂rseller
  if (referenceImages && referenceImages.length > 0) {
    referenceImages.forEach(base64Str => {
      const base64Data = base64Str.split(',')[1];
      let mimeType = 'image/jpeg';
      if (base64Str.startsWith('data:image/png')) mimeType = 'image/png';
      if (base64Str.startsWith('data:image/webp')) mimeType = 'image/webp';
      parts.push({ inlineData: { mimeType: mimeType, data: base64Data } });
    });
  }
  
  parts.push({ text: prompt });

  // --- MODEL CALL WITH FALLBACK LOGIC ---
  let response;

  try {
    // 1. Asƒ±l modeli dene (√ñrn: Super Pro)
    response = await ai.models.generateContent({
      model: targetImageModel,
      contents: { parts: parts },
      config: { imageConfig: imageConfig }
    });

  } catch (error: any) {
    // 2. Hata yakalama ve Fallback
    const errMsg = error.message || error.toString();
    
    // Eƒüer SUPER PRO kullanƒ±yorsak ve Yetki (403) veya Bulunamadƒ± (404) hatasƒ± alƒ±rsak
    if ((modelId === 'syko-super-pro') && (errMsg.includes("403") || errMsg.includes("404") || errMsg.includes("not found") || errMsg.includes("permission"))) {
      try {
        // Sessizce FLASH modeline ge√ß (Kurtarma Operasyonu)
        response = await ai.models.generateContent({
          model: 'gemini-2.5-flash-image', // G√ºvenli Liman
          contents: { parts: parts },
          config: { imageConfig: { aspectRatio: '1:1' } } // Basit config
        });
      } catch (retryError: any) {
        // O da √ßalƒ±≈ümazsa artƒ±k yapacak bir ≈üey yok, hatayƒ± fƒ±rlat
        throw new Error(`G√∂rsel olu≈üturulamadƒ± (Fallback ba≈üarƒ±sƒ±z): ${retryError.message}`);
      }
    } else {
      // Diƒüer modellerde veya ba≈üka hatalarda direkt hatayƒ± fƒ±rlat
      throw error;
    }
  }

  // --- SONU√á ƒ∞≈ûLEME ---
  try {
    const generatedImages: string[] = [];
    let generatedText = "";

    if (response && response.candidates && response.candidates[0].content && response.candidates[0].content.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.inlineData) {
          const base64Str = part.inlineData.data;
          const mimeType = part.inlineData.mimeType || 'image/png';
          generatedImages.push(`data:${mimeType};base64,${base64Str}`);
        } else if (part.text) {
          generatedText += part.text;
        }
      }
    }

    if (generatedImages.length === 0) {
       if (generatedText) {
          return { text: generatedText, images: [] };
       }
       throw new Error("G√∂rsel olu≈üturulamadƒ± (Model bo≈ü yanƒ±t d√∂nd√º).");
    }

    return { text: generatedText, images: generatedImages };

  } catch (error: any) {
    const errMsg = error.message || error.toString();
    if (errMsg.includes("404")) throw new Error("Model hatasƒ± (404).");
    if (errMsg.includes("400")) throw new Error("ƒ∞stek reddedildi (400). ƒ∞√ßerik politikasƒ±.");
    if (errMsg.includes("429")) throw new Error("API Kotasƒ± a≈üƒ±ldƒ± (429).");
    
    throw new Error(`${errMsg}`);
  }
};

// ============================================================================
// üîå NGROK STREAMING
// ============================================================================
async function streamNgrokResponse(
  history: Message[], 
  onChunk: (text: string) => void,
  signal?: AbortSignal,
  images?: string[]
): Promise<string> {
  if (CUSTOM_MODEL_CONFIG.baseURL.includes("CHANGE_THIS") || !CUSTOM_MODEL_CONFIG.baseURL.startsWith("http")) {
    throw new Error("‚ö†Ô∏è AYAR EKSƒ∞K: Ngrok URL kontrol et.");
  }

  const messages: any[] = history.map(msg => ({
    role: msg.role === 'model' ? 'assistant' : 'user',
    content: msg.content
  }));

  if (images && images.length > 0) {
     const lastMsgIndex = messages.length - 1;
     const textContent = messages[lastMsgIndex].content;
     const contentArray: any[] = [{ type: "text", text: textContent }];
     images.forEach(img => {
         contentArray.push({ type: "image_url", image_url: { url: img } });
     });
     messages[lastMsgIndex].content = contentArray;
  }

  const systemMessage = {
    role: "system",
    content: `You are SykoLLM NATIVE.
    ${NATURAL_LANGUAGE_PROTOCOL}
    ${SHARED_THINKING_PROTOCOL}
    `
  };
  
  const payload = {
    model: CUSTOM_MODEL_CONFIG.modelId,
    messages: [systemMessage, ...messages],
    stream: true, 
    temperature: 0.7,
    max_tokens: 4096
  };

  try {
    const response = await fetch(`${CUSTOM_MODEL_CONFIG.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${CUSTOM_MODEL_CONFIG.apiKey}`,
        'ngrok-skip-browser-warning': 'true'
      },
      body: JSON.stringify(payload),
      signal: signal
    });

    if (!response.ok) {
        if (response.status === 400 && images && images.length > 0) {
            throw new Error("bu model g√∂rsel g√∂rmeyi (vision) desteklememektedir."); 
        }
        const errorText = await response.text();
        throw new Error(`Local Error: ${errorText}`);
    }
    if (!response.body) throw new Error("Empty body");
    const reader = response.body.getReader();
    const decoder = new TextDecoder("utf-8");
    let fullText = "";
    let buffer = "";

    while (true) {
      if (signal?.aborted) { reader.cancel(); break; }
      const { done, value } = await reader.read();
      if (done) break;
      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split("\n");
      buffer = lines.pop() || ""; 
      for (const line of lines) {
        const trimmed = line.trim();
        if (!trimmed.startsWith("data: ")) continue;
        const dataStr = trimmed.slice(6);
        if (dataStr === "[DONE]") continue;
        try {
          const json = JSON.parse(dataStr);
          const content = json.choices?.[0]?.delta?.content || "";
          if (content) { fullText += content; onChunk(content); }
        } catch (e) { }
      }
    }
    return fullText;
  } catch (error: any) {
    if (error.name === 'AbortError') return " [ABORTED]";
    throw new Error(error.message);
  }
}

// ============================================================================
// üöÄ ANA STREAM HANDLER (TEXT CHAT)
// ============================================================================

export const streamResponse = async (
  modelId: string, 
  history: Message[],
  onChunk: (text: string) => void,
  signal?: AbortSignal,
  images?: string[] 
): Promise<string> => {
  
  if (modelId === 'syko-native') {
    return await streamNgrokResponse(history, onChunk, signal, images);
  }

  const apiKey = process.env.API_KEY;
  if (!apiKey) throw new Error("API Key eksik!");
  
  const ai = new GoogleGenAI({ apiKey });

  try {
    let targetModel = "gemini-2.5-flash"; 
    let thinkingBudget = 0; // Default: D√º≈ü√ºnme kapalƒ±
    
    // --- MODEL KONFƒ∞G√úRASYONU VE D√ú≈û√úNME AYARLARI ---
    if (modelId === 'syko-v2.5') {
        targetModel = "gemini-2.5-flash"; 
        thinkingBudget = 0; // V2.5 i√ßin d√º≈ü√ºnme kapatƒ±ldƒ± (Hata √∂nleme)
    } else if (modelId === 'syko-v3-pro') {
        targetModel = "gemini-3-flash-preview"; 
        thinkingBudget = 4096; // Orta seviye d√º≈ü√ºnme
    } else if (modelId === 'syko-super-pro') {
        targetModel = "gemini-3-pro-preview"; 
        thinkingBudget = 8192; // Y√ºksek seviye d√º≈ü√ºnme (Derin Analiz)
    }

    const systemInstruction = SYSTEM_PROMPTS[modelId] || SYSTEM_PROMPTS['syko-v2.5'];

    const historyContents = history.slice(0, -1).map(msg => ({
      role: msg.role === 'user' ? 'user' : 'model',
      parts: [{ text: msg.content }]
    }));

    const lastMessage = history[history.length - 1];
    const currentParts: any[] = [];
    
    if (images && images.length > 0) {
      images.forEach(base64Str => {
        const base64Data = base64Str.split(',')[1];
        let mimeType = 'image/jpeg';
        if (base64Str.startsWith('data:image/png')) mimeType = 'image/png';
        if (base64Str.startsWith('data:image/webp')) mimeType = 'image/webp';
        currentParts.push({ inlineData: { mimeType: mimeType, data: base64Data } });
      });
    }

    currentParts.push({ text: lastMessage.content });

    // CONFIG G√úNCELLENDƒ∞: thinkingConfig sadece b√ºt√ße varsa eklenir
    const generateConfig: any = {
        systemInstruction: systemInstruction
    };

    if (thinkingBudget > 0) {
        generateConfig.thinkingConfig = { thinkingBudget: thinkingBudget };
    }

    const responseStream = await ai.models.generateContentStream({
      model: targetModel,
      contents: [ ...historyContents, { role: 'user', parts: currentParts } ],
      config: generateConfig,
    });

    let fullText = "";

    for await (const chunk of responseStream) {
      if (signal?.aborted) break;
      const text = chunk.text;
      if (text) {
        fullText += text;
        onChunk(text);
      }
    }
    return fullText;

  } catch (error: any) {
    if (error.name === 'AbortError') return "[ABORTED]";
    throw new Error(error.message || "Bilinmeyen bir hata olu≈ütu.");
  }
};